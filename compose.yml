volumes:
  mariadbdata: {}
  ollama: {}
  qdrant_storage: {}
  mongodata_db: {}
  mongodata_config: {}
  redis: {}
  open_webui: {}
  trained: {}
  hugging_face: {}
services:
  redis:
    image: redis
    ports:
      - '6379:6379'
    restart: always
    volumes:
      - redis:/data
  ollama:
    image: ollama/ollama
    ports:
      - '11434:11434'
    post_start:
      - command: "ollama rm $LLM_MODEL || true;"
      - command: "while [! -f \"/trained/$LLM_MODEL-unsloth.gguf\" ]; do sleep 1; done"
      - command: "ollama create $LLM_MODEL -f /trained/$LLM_MODEL-unsloth.gguf"
      - command: "ollama run $LLM_MODEL"
    restart: always
    runtime: nvidia
    env_file:
      - .env
    volumes:
      - ollama:/root/.ollama
      - trained:/trained
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
              - gpu
  qdrant:
    image: qdrant/qdrant
    ports:
      - '6333:6333'
      - '6334:6334'
    restart: always
    volumes:
      - qdrant_storage:/qdrant/storage
  mariadb:
    image: mariadb
    ports:
      - '3306:3306'
    restart: always
    environment:
      MARIADB_ROOT_PASSWORD: example
    volumes:
      - mariadbdata:/var/lib/mysql
  mongo:
    image: mongo
    restart: always
    ports:
      - '27017:27017'
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
    volumes:
      - mongodata_db:/data/db
      - mongodata_config:/data/configdb
  app:
    build:
      dockerfile: App/Dockerfile
      no_cache: true
    env_file:
      - .env
    ports:
      - '80:80'
    depends_on:
      - ollama
      - qdrant
      - mariadb
      - mongo
      - redis
  open-webui:
    image: ghcr.io/open-webui/open-webui:ollama
    restart: always
    ports:
      - '8080:3000'
    environment:
      OLLAMA_BASE_URL: "http://ollama:11434"
    volumes:
      - open_webui:/app/backend/data
    depends_on:
      - ollama
  training:
    build:
      dockerfile: Training/Dockerfile
      no_cache: true
    env_file:
      - .env
    volumes:
      - trained:/training/outputs
      - hugging_face:/hugging_face_cache
    runtime: nvidia
    environment:
      CUDA_LAUNCH_BLOCKING: 1
      HF_HOME: /hugging_face_cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities:
              - gpu